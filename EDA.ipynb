{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import pickle\n","import random\n","import time\n","import json\n","\n","import cv2\n","import matplotlib.patches as patches\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","from DataPrep import DataPrep\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"read 5 frames runtime:  4.020122528076172\n(5, 1080, 1920, 3) (4, 1080, 1920, 2) 5 4\n"}],"source":["# for _ in range(10):\n","dp = DataPrep(segment_size=5)\n","datapath = '/home/alex/projects/DeepFakeDetection/data/train_sample_videos'\n","vid = os.path.join(datapath, random.choice(os.listdir(datapath)))\n","start = time.time()\n","frames = dp.getFrameSnippet(vid, start_frame=0)\n","flows = dp.getOpticalFlows(frames)\n","rgb_rois = []\n","flow_rois = []\n","for i in range(int(frames.shape[0])):\n","    frame = frames[i]\n","    if i > 0:\n","        flow = flows[i - 1]\n","        rgb_faces, flow_faces = dp.getFaces(frame, flow=flow)\n","    else:\n","        rgb_faces, flow_faces = dp.getFaces(frame)\n","    rgb_rois.extend(rgb_faces)\n","    flow_rois.extend(flow_faces)\n","flow_rois = [r for r in flow_rois if r is not None]\n","rgb = np.stack(rgb_rois)\n","flow = np.stack(flow_rois)\n","print(f\"read {dp.segment_size} frames runtime: \", time.time() - start)\n","print(frames.shape, flows.shape, len(rgb_rois), len(flow_rois))"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>split</th>\n      <th>original</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>aagfhgtpmv.mp4</th>\n      <td>FAKE</td>\n      <td>train</td>\n      <td>vudstovrck.mp4</td>\n    </tr>\n    <tr>\n      <th>aapnvogymq.mp4</th>\n      <td>FAKE</td>\n      <td>train</td>\n      <td>jdubbvfswz.mp4</td>\n    </tr>\n    <tr>\n      <th>abarnvbtwb.mp4</th>\n      <td>REAL</td>\n      <td>train</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>abofeumbvv.mp4</th>\n      <td>FAKE</td>\n      <td>train</td>\n      <td>atvmxvwyns.mp4</td>\n    </tr>\n    <tr>\n      <th>abqwwspghj.mp4</th>\n      <td>FAKE</td>\n      <td>train</td>\n      <td>qzimuostzz.mp4</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"               label  split        original\naagfhgtpmv.mp4  FAKE  train  vudstovrck.mp4\naapnvogymq.mp4  FAKE  train  jdubbvfswz.mp4\nabarnvbtwb.mp4  REAL  train            None\nabofeumbvv.mp4  FAKE  train  atvmxvwyns.mp4\nabqwwspghj.mp4  FAKE  train  qzimuostzz.mp4"},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["metadata = pd.read_json('data/train_sample_videos/metadata.json').T\n","metadata.head()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"FAKE 0.6191950464396285\nREAL 2.5974025974025974\n"}],"source":["class_weights = compute_class_weight('balanced', np.unique(metadata.label.values), metadata.label.values)\n","for k,v in zip(np.unique(metadata.label.values), class_weights):\n","    print(k,v)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":"(128, 128, 3)"},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["rgb.shape[1:]"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["rgb_input = tf.keras.Input(shape=rgb.shape[1:])\n","flow_input = tf.keras.Input(shape=flow.shape[1:])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x = tf.keras.layers.Convolution2D(\n","    filters=rgb.shape[0],\n","    \n","    )"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["counter = 0\n","for k, v in labels.items():\n","    if v['split'] == 'test':\n","        counter += 1\n","counter\n","\n"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["fcPath = '/home/alex/data/opencv/data/haarcascades'\n","\n","frontface = 'haarcascade_frontalface_default.xml'\n","profileface = 'haarcascade_profileface.xml'\n","FF = cv2.CascadeClassifier(os.path.join(fcPath, frontface))\n","FP = cv2.CascadeClassifier(os.path.join(fcPath, profileface))"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["datapath = '/home/alex/projects/DeepFakeDetection/data/train_sample_videos'\n","vid = os.path.join(datapath, random.choice(os.listdir(datapath)))\n","num_frames = 5\n","start = time.time()\n","frames = getFrameSnippet(vid, num_frames=num_frames)\n","flows = getOpticalFlows(frames)\n","rgb_rois = []\n","flow_rois = []\n","for i in range(int(frames.shape[0])):\n","    frame = frames[i]\n","    if i > 0:\n","        flow = flows[i - 1]\n","        rgb_faces, flow_faces = getFaces(frame, flow=flow)\n","    else:\n","        rgb_faces, flow_faces = getFaces(frame)\n","    rgb_rois.extend(rgb_faces)\n","    flow_rois.extend(flow_faces)\n","print(f\"read {num_frames} frames runtime: \", time.time() - start)\n","print(frames.shape, flows.shape, len(rgb_rois), len(flow_rois))"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["for roi in rgb_rois:\n","    print(roi.shape)\n","for roi in flow_rois:\n","    print(roi.shape)"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["datapath = '/home/alex/projects/DeepFakeDetection/data/train_sample_videos'\n","vid = os.path.join(datapath, random.choice(os.listdir(datapath)))\n","num_frames = 5\n","start = time.time()\n","frames = getFrameSnippet(vid, num_frames=num_frames)\n","print(f\"read {num_frames} frames runtime: \", time.time() - start)\n","print(frames.shape)\n","start = time.time()\n","prvs = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)\n","flows = np.empty((frames.shape[0] - 1, frames.shape[1], frames.shape[2], 2))\n","for i in range(1, int(frames.shape[0])):\n","    frame = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n","    flow = cv2.calcOpticalFlowFarneback(\n","        prvs, frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n","    print(flow.shape)\n","print(\"flow calculation runtime: \", time.time() - start)\n","\n"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["frames = getFrameSnippet(vid)"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["dir(cv2)"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["cap.set\n","ret, frame = cap.read()\n","gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","faces = FF.detectMultiScale(\n","    image=gray,\n","    scaleFactor=1.6,\n","    minNeighbors=2)\n","if len(faces) < 1:\n","    faces = FP.detectMultiScale(\n","        image=gray,\n","        scaleFactor=1.3,\n","        minNeighbors=1)\n","# faces = list(facesfront) + list(facesprofile)\n","# face = sorted(faces, key=lambda x: -x[2])[0]\n","# x,y,w,h = face\n","\n","fig, ax = plt.subplots()\n","ax.imshow(frame)\n","for x, y, w, h in faces:\n","    rect = patches.Rectangle(\n","        (x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n","    ax.add_patch(rect)"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["rois = []\n","for x, y, w, h in faces:\n","    face = frame[y:y + h, x:x + w, :]\n","    rois.append(face)\n","for roi in rois:\n","    plt.imshow(roi)"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["len(rois)\n"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["plt.imshow(face)"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["face"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["folders = []\n","for dirname, subdirs, files in os.walk('/home/alex/anaconda3'):\n","    if 'opencv' in dirname.lower():\n","        folders.append(dirname)\n","        break\n","folders\n"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["\n","\n","# _, frame = cap.read()\n","\n","# # # convert to grayscale\n","# # gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","# # detect faces\n","# faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n","\n","# # for each detected face\n","# for (x, y, w, h) in faces:\n","#     # create rectangle around it\n","#     cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 255, 0), 2)\n","\n","#     # crop the ROI\n","#     roi = gray[y:y + h, x:x + w]\n","\n","#     # encode as png, convert to bytes\n","#     _, img = cv2.imencode('.png', roi)\n","#     msg = img.tobytes()\n","\n","#     # publish the message\n","#     client.publish(topic, msg, 0)\n","\n","#     # record that a face has been captured\n","#     counter += 1\n"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.5"},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}